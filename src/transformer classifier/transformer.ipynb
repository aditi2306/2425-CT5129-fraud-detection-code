{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer as classifier on raw dataset"
      ],
      "metadata": {
        "id": "lbKDXqfpaJCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPVPJAkbRvHr",
        "outputId": "5f8c6cf2-5077-4dd2-c172-4fafd6756802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1422/1422 - 839s - 590ms/step - auc: 0.9663 - loss: 0.1962 - precision: 0.9499 - recall: 0.8616 - val_auc: 0.0000e+00 - val_loss: 0.2269 - val_precision: 1.0000 - val_recall: 0.8960\n",
            "Epoch 2/20\n",
            "1422/1422 - 860s - 605ms/step - auc: 0.9912 - loss: 0.1095 - precision: 0.9633 - recall: 0.9195 - val_auc: 0.0000e+00 - val_loss: 0.1205 - val_precision: 1.0000 - val_recall: 0.9338\n",
            "Epoch 3/20\n",
            "1422/1422 - 792s - 557ms/step - auc: 0.9974 - loss: 0.0640 - precision: 0.9716 - recall: 0.9614 - val_auc: 0.0000e+00 - val_loss: 0.0736 - val_precision: 1.0000 - val_recall: 0.9715\n",
            "Epoch 4/20\n",
            "1422/1422 - 831s - 585ms/step - auc: 0.9987 - loss: 0.0383 - precision: 0.9805 - recall: 0.9849 - val_auc: 0.0000e+00 - val_loss: 0.0334 - val_precision: 1.0000 - val_recall: 0.9871\n",
            "Epoch 5/20\n",
            "1422/1422 - 834s - 587ms/step - auc: 0.9992 - loss: 0.0264 - precision: 0.9856 - recall: 0.9939 - val_auc: 0.0000e+00 - val_loss: 0.0055 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 6/20\n",
            "1422/1422 - 849s - 597ms/step - auc: 0.9994 - loss: 0.0175 - precision: 0.9906 - recall: 0.9980 - val_auc: 0.0000e+00 - val_loss: 0.0056 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 7/20\n",
            "1422/1422 - 850s - 598ms/step - auc: 0.9995 - loss: 0.0144 - precision: 0.9924 - recall: 0.9985 - val_auc: 0.0000e+00 - val_loss: 0.0138 - val_precision: 1.0000 - val_recall: 0.9975\n",
            "Epoch 8/20\n",
            "1422/1422 - 782s - 550ms/step - auc: 0.9995 - loss: 0.0135 - precision: 0.9930 - recall: 0.9982 - val_auc: 0.0000e+00 - val_loss: 0.0083 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 9/20\n",
            "1422/1422 - 779s - 547ms/step - auc: 0.9997 - loss: 0.0105 - precision: 0.9943 - recall: 0.9987 - val_auc: 0.0000e+00 - val_loss: 0.0016 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 10/20\n",
            "1422/1422 - 852s - 599ms/step - auc: 0.9997 - loss: 0.0097 - precision: 0.9950 - recall: 0.9988 - val_auc: 0.0000e+00 - val_loss: 6.8911e-04 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 11/20\n",
            "1422/1422 - 806s - 567ms/step - auc: 0.9997 - loss: 0.0084 - precision: 0.9957 - recall: 0.9991 - val_auc: 0.0000e+00 - val_loss: 0.0014 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 12/20\n",
            "1422/1422 - 802s - 564ms/step - auc: 0.9997 - loss: 0.0075 - precision: 0.9962 - recall: 0.9991 - val_auc: 0.0000e+00 - val_loss: 9.9655e-04 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 13/20\n",
            "1422/1422 - 874s - 614ms/step - auc: 0.9998 - loss: 0.0071 - precision: 0.9964 - recall: 0.9991 - val_auc: 0.0000e+00 - val_loss: 0.0024 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 14/20\n",
            "1422/1422 - 853s - 600ms/step - auc: 0.9998 - loss: 0.0063 - precision: 0.9967 - recall: 0.9994 - val_auc: 0.0000e+00 - val_loss: 0.0052 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 15/20\n",
            "1422/1422 - 871s - 612ms/step - auc: 0.9998 - loss: 0.0063 - precision: 0.9968 - recall: 0.9993 - val_auc: 0.0000e+00 - val_loss: 0.0033 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 16/20\n",
            "1422/1422 - 805s - 566ms/step - auc: 0.9998 - loss: 0.0063 - precision: 0.9968 - recall: 0.9992 - val_auc: 0.0000e+00 - val_loss: 0.0014 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 17/20\n",
            "1422/1422 - 835s - 587ms/step - auc: 0.9998 - loss: 0.0059 - precision: 0.9970 - recall: 0.9992 - val_auc: 0.0000e+00 - val_loss: 0.0023 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 18/20\n",
            "1422/1422 - 806s - 566ms/step - auc: 0.9998 - loss: 0.0053 - precision: 0.9973 - recall: 0.9995 - val_auc: 0.0000e+00 - val_loss: 0.0010 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 19/20\n",
            "1422/1422 - 806s - 567ms/step - auc: 0.9998 - loss: 0.0046 - precision: 0.9976 - recall: 0.9995 - val_auc: 0.0000e+00 - val_loss: 0.0024 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 20/20\n",
            "1422/1422 - 862s - 606ms/step - auc: 0.9998 - loss: 0.0049 - precision: 0.9976 - recall: 0.9993 - val_auc: 0.0000e+00 - val_loss: 1.6858e-04 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "\u001b[1m1781/1781\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 21ms/step\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.39      0.86      0.54        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.70      0.93      0.77     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n",
            "Test ROC‑AUC: 0.9578\n",
            "Test PR‑AUC : 0.7618\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "X = df.drop('Class', axis=1).values\n",
        "y = df['Class'].values\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# Oversample the minority class (fraud) to mitigate imbalance\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_bal, y_train_bal = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "n_features = X_train_bal.shape[1]\n",
        "d_model    = 64\n",
        "num_heads  = 4\n",
        "ff_dim     = 128\n",
        "num_layers = 2\n",
        "\n",
        "inputs = tf.keras.Input(shape=(n_features,))\n",
        "# Reshape each sample to a \"sequence\" of feature tokens\n",
        "x = layers.Reshape((n_features, 1))(inputs)\n",
        "# Project to d_model dimensions\n",
        "x = layers.Dense(d_model)(x)\n",
        "\n",
        "\n",
        "positions = tf.range(start=0, limit=n_features, delta=1)\n",
        "pos_embed = layers.Embedding(input_dim=n_features, output_dim=d_model)\n",
        "x = x + pos_embed(positions)\n",
        "\n",
        "# Transformer encoder blocks\n",
        "for _ in range(num_layers):\n",
        "\n",
        "    attn_output = layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                            key_dim=d_model)(x, x)\n",
        "    x = layers.Add()([x, attn_output])\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    # Feed-forward network\n",
        "    ff_output = Sequential([\n",
        "        layers.Dense(ff_dim, activation='relu'),\n",
        "        layers.Dense(d_model),\n",
        "    ])(x)\n",
        "    x = layers.Add()([x, ff_output])\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "neg, pos = np.bincount(y_train_bal)\n",
        "class_weight = {0: 1.0, 1: neg / pos}\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        tf.keras.metrics.AUC(name='auc'),\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_bal, y_train_bal,\n",
        "    epochs=20,\n",
        "    batch_size=256,\n",
        "    validation_split=0.2,\n",
        "    class_weight=class_weight,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "y_prob = model.predict(X_test).ravel()\n",
        "y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "pr_auc  = average_precision_score(y_test, y_prob)\n",
        "print(f\"Test ROC‑AUC: {roc_auc:.4f}\")\n",
        "print(f\"Test PR‑AUC : {pr_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note-Chatgpt was used to undersatnd results and solve errors in encoder block"
      ],
      "metadata": {
        "id": "k9BiC6B7aurg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer model, when trained on the original highly imbalanced dataset, achieves very high overall accuracy (≈100%) and a strong ROC-AUC of 0.9578, reflecting excellent discrimination between fraudulent and legitimate transactions.\n",
        "\n",
        "However, looking closer at fraud detection:\n",
        "\n",
        "Recall for fraud (class 1) is high at 0.86, meaning the model is able to catch the majority of fraudulent cases.\n",
        "\n",
        "Precision for fraud is relatively low at 0.39, indicating that a substantial proportion of predicted fraud cases are false positives.\n",
        "\n",
        "The F1-score for fraud is 0.54, showing only moderate balance between precision and recall.\n",
        "\n",
        "The PR-AUC of 0.7618 further confirms that while the model is effective at ranking instances, its performance under imbalanced conditions still favors the majority class.\n",
        "\n",
        "In other words, although the Transformer excels at distinguishing fraud from non-fraud in terms of ranking (ROC-AUC), the imbalance in the dataset limits its ability to maintain high precision for the minority fraud class.\n",
        "\n",
        "Overall, the Transformer shows strong potential with excellent recall, making it effective at catching fraud, but improvements (such as data augmentation, threshold tuning, or hybrid GAN-based oversampling) are needed to boost precision and practical deployment performance."
      ],
      "metadata": {
        "id": "W5vbOCa1bb74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Credit card fraud detection using advanced transformer model](https://arxiv.org/abs/2402.00000), https://www.linkedin.com/pulse/credit-card-fraud-detection-how-ai-powered-algorithms-utpal-dutta-f07sc/"
      ],
      "metadata": {
        "id": "2ubwixqzbACz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S8B41KP1a22w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}